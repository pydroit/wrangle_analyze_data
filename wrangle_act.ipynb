{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "# Project: Wrangling and Analyze Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering\n",
    ">In the **Data Gathering** section, the data needed for the project is gathered and made available for assessment, cleaning and analysis.\n",
    "\n",
    "The methods required to gather each of the three data are different.\n",
    "1. **WeRateDogs Twitter Archive Data:** This is manually downloaded and uploaded to Jupyter Notebook. File name is `twitter-archive-enhanced.csv`.\n",
    "\n",
    "2. **Tweet Image Prediction:** Though this is a `tsv` file, it is hosted on Udacity's servers and is to be programmatically downloaded. The Requests library is used to download the file `Ã¬mage_predictions.tsv` from this URL [here](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv).\n",
    "\n",
    "3. **Additional data from Tweepy library via the Twitter API (tweet_json.txt):** Additional data is queried from the twitter API to add more details and analysis to the report. `Retweet Count` and `Favorite Count` by Twitter IDs will be queried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import needed packages\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import requests\n",
    "import tweepy\n",
    "import json\n",
    "from functools import reduce\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. WeRateDogs Twitter Archive Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the twitter-archive-enhanced.csv file\n",
    "\n",
    "twitter_archive = pd.read_csv('twitter-archive-enhanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visually assess the data\n",
    "\n",
    "twitter_archive.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Programmatically assess the data\n",
    "\n",
    "twitter_archive.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tweet Image Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download tsv file programmatically\n",
    "\n",
    "url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "response = requests.get(url)\n",
    "\n",
    "with open('image-predictions.tsv', mode = 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Load the file into a pandas DataFrame\n",
    "\n",
    "image_prediction_file = pd.read_csv('image-predictions.tsv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually assess the image_prediction_file file\n",
    "\n",
    "image_prediction_file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Additional data from Tweepy library via the Twitter API (tweet_json.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tweepy\n",
    "\n",
    "\n",
    "# consumer_key = ''\n",
    "# consumer_secret = ''\n",
    "# access_token = ''\n",
    "# access_secret = ''\n",
    "\n",
    "# auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "# auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "# api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fail_message = []\n",
    "# with open('tweet_json.txt', 'a', encoding='utf8') as f:\n",
    "#     for tweet_id in twitter_archive['tweet_id']:\n",
    "#         try:\n",
    "#             tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
    "#             tweet_json = json.dumps(tweet._json)\n",
    "#             f.write(tweet_json + '\\n')\n",
    "#         except:\n",
    "#             print(f\"Retrieving tweet with ID: {tweet_id} failed\")\n",
    "#             fail_message.append(tweet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "with open('tweet_json.txt', mode='r') as f:\n",
    "    #f = file.readlines()\n",
    "    for each_line in f:\n",
    "        tweet = json.loads(each_line)\n",
    "        tweet_id = tweet['id']\n",
    "        retweet_count = tweet['retweet_count']\n",
    "        favorite_count = tweet['favorite_count']\n",
    "        df.append({'tweet_id' : tweet_id,\n",
    "            'retweet_count' : retweet_count,\n",
    "                  'favorite_count' : favorite_count})\n",
    "        \n",
    "tweet_json_data = pd.DataFrame(df, columns = ['tweet_id', 'retweet_count', 'favorite_count'\n",
    "                                  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 28,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Assessing Data\n",
    "\n",
    "> In detecting quality and tidiness issues in the three datasets, the data can be assessed both visually and programmatically.\n",
    "\n",
    "1. **Visual Assessment** comes to play when Data is 'previewed' within the interface of the Jupyter notebook. We can get a quick view of what the data looks like, but could be cumbersome having a view of the whole data, especially for large datasets.\n",
    "2. **Programmatical Assessment** is when pandas functions and methods are used to assess the data and have a grasp of what the data is.\n",
    "\n",
    "For the purpose of this analysis, **eight (8) quality issues and two (2) tidiness issues** will be identified programmatically and visually from the data.\n",
    "It is also important to note that only original ratings that have image will be analysed. Retweets will be excluded from our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing dataset 1 `twitter_archive` to detect quality and/or tidiness issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive['source'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive['name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing dataset 2 `image_prediction_file` to detect quality and/or tidiness issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_prediction_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_prediction_file.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_prediction_file.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing dataset 3 `tweet_json` to detect quality and/or tidiness issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality issues\n",
    "1. Since only original ratings are needed, the replies (`in_reply_to_status_id`, count = 78) and retweets (`retweeted_status_id`, count = 181) in `twitter_archive` is not necessary for the analysis.\n",
    "\n",
    "\n",
    "2. `image_prediction_file` has a count of 2075 while `twitter_archive` has 2356 rows. Tweets that have no image and even retweets/replies must have been included in the `twitter_archive` data.\n",
    "\n",
    "\n",
    "3. Since there's no need for retweets and replies, some columns are extraneous on the `twitter_archive` data.\n",
    "The columns that needs to be removed are **`in_reply_to_status_id`**, **`in_reply_to_user_id`**, **`retweeted_status_id`**, **`retweeted_status_user_id`** and **`retweeted_status_timestamp`**.\n",
    "\n",
    "\n",
    "\n",
    "4. The **`timestamp`** column on the `twitter_archive` has a consistent tailing '+0000' which is unnecessary.\n",
    "\n",
    "\n",
    "5. The **`source`** column in the `twitter_archive` dataset still has unneeded html elements which make the column not easy to read and use.\n",
    "\n",
    "\n",
    "6. The timestamp field has the wrong datatype. This is to be converted to datetime.\n",
    "\n",
    "\n",
    "7. Inconsistent case in the **`p1`**, **`p2`** and **`p3`** columns as some are lower case while some are sentence case.\n",
    "\n",
    "\n",
    "8. Some columns do not have good descriptive names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 7,
        "hidden": false,
        "row": 40,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "### Tidiness issues\n",
    "1. The analysis will be cleaner and easier if the three datasets (`twitter_archive`, `image_prediction_file`, `tweet_json_data`) were merged into a single DataFrame.\n",
    "\n",
    "2. The **`doggo`**, **`floofer`**, **`pupper`** and **`puppo`** columns on the `twitter_archive` dataset could actually be Melt into a single column and called say, 'dog_type'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 32,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Cleaning Data\n",
    "\n",
    "> The data quality and tidiness issues have been identified. Now is time to do the actual cleaning and aligning of the data.\n",
    "In this section, **all** quality and tidiness issues will be treated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making copies of original datasets\n",
    "\n",
    "The cleaning process will involve making a lot of changes to the original datasets. It is appropriate to make copies of the original pieces of data so that work is done only on these copies.\n",
    "\n",
    "**Note:** _The cleaning process would not sequentially follow the order in which issues are outlined above, but possibly in the order of how a step takes precedence over the next._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make copies of the original datasets\n",
    "\n",
    "twitter_archive_clean = twitter_archive.copy()\n",
    "image_prediction_file_clean = image_prediction_file.copy()\n",
    "tweet_json_data_clean = tweet_json_data.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #1: \n",
    "\n",
    "The analysis will be cleaner and easier if the three datasets (`twitter_archive`, `image_prediction_file`, `tweet_json_data`) were merged into a single DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define:\n",
    "\n",
    "It is tidy to organise the individual datasets into just one. Working on one dataset instead of three ensures we do not make repeatitive actions on each of the datasets, and morever, the data we are working on is really just one data derived from different sources.\n",
    "These resources; [Real Python](https://realpython.com/pandas-merge-join-and-concat/), [Documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#database-style-dataframe-or-named-series-joining-merging) and [Stackoverflow](https://stackoverflow.com/questions/44327999/python-pandas-merge-multiple-dataframes) facilitated my understanding of the pandas merging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All three datasets have a similar column (tweet_id) that can be used as primary key to merge\n",
    "# The three datasets are combined at same time using 'reduce' and 'merge'\n",
    "combined_data = [twitter_archive_clean, image_prediction_file_clean, tweet_json_data_clean]\n",
    "\n",
    "merged_df = reduce(lambda  left,right: pd.merge(left,right,on=['tweet_id'], how='outer'\n",
    "                                            ), combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "merged_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm all three datasets are merged into one\n",
    "\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #2: \n",
    "\n",
    "Data contains reply and retweets which are not needed in the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define:\n",
    "\n",
    "Since only original ratings are needed, the replies (`in_reply_to_status_id`, count = 78) and retweets (`retweeted_status_id`, count = 181) in the `merged_df` is not necessary for the analysis.<br><br>Solving this problem, unnecessary items will be removed from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows that are actually replies. \n",
    "# Rows that have the 'in_reply_to_status_id' populated are replies.\n",
    "# The method used is retaining only rows where the 'in_reply_to_status_id' is null\n",
    "\n",
    "merged_df = merged_df[merged_df.in_reply_to_status_id.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows that are actually retweets.\n",
    "# Rows that have the 'retweeted_status_id' populated are retweets.\n",
    "# The method used is retaining only rows where the 'retweeted_status_id' is null\n",
    "\n",
    "merged_df = merged_df[merged_df.retweeted_status_id.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the retweets and replies are no more in the dataset\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #3:\n",
    "\n",
    "Removing tweets that have no image in the `image_prediction_file` data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "source": [
    "#### Define\n",
    "\n",
    "We can see from `merged_df` that the **`jpg_url`** field which belonged to the `image_prediction_file` has 1971 rows while the other records that belonged to `twiter_archive` and `twee_json` has a count of 2097. This implies Tweets that have no image and even retweets/replies must have been included in the `twitter_archive` data.\n",
    "\n",
    "In solving this issue, only rows that have images will be retained so that those that do not have images are discarded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows that do not have images\n",
    "# This is done by retaining rows with image\n",
    "\n",
    "merged_df = merged_df[merged_df.jpg_url.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confirm that records without image are no more in the dataset\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #4:\n",
    "\n",
    "Since there's no need for retweets and replies, some columns are extraneous on the `merged_df` data.\n",
    "The columns that needs to be removed are **`in_reply_to_status_id`**, **`in_reply_to_user_id`**, **`retweeted_status_id`**, **`retweeted_status_user_id`** and **`retweeted_status_timestamp`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unneeded columns\n",
    "\n",
    "merged_df = merged_df.drop(['in_reply_to_status_id', 'in_reply_to_user_id', 'retweeted_status_id',\n",
    "                            'retweeted_status_user_id', 'retweeted_status_timestamp'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm if the dropped columns are no more in the dataset\n",
    "\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #5:\n",
    "\n",
    "The **`timestamp`** column has a consistent tailing '+0000' which is unnecessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the trailing '+0000' using split\n",
    "\n",
    "merged_df.timestamp = merged_df.timestamp.str.strip('+0000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confirm if the trailing '+0000' has been removed\n",
    "\n",
    "merged_df['timestamp'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #6:\n",
    "\n",
    "The timestamp field has the wrong datatype.\n",
    "This is to be converted to datetime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the timestamp datatype to datetime\n",
    "\n",
    "merged_df.timestamp = pd.to_datetime(merged_df.timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confirm if the datatype has been converted to datetime\n",
    "\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #7:\n",
    "\n",
    "The **`source`** column still has unneeded html elements which make the column not easy to read and use.\n",
    "\n",
    "This will be cleaned-up using regex function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the column to confirm how best to clean it\n",
    "\n",
    "merged_df['source'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the trailing '+0000' using split\n",
    "# Convert to datetime\n",
    "\n",
    "merged_df['source'] = merged_df['source'].apply(lambda _: re.findall(r'>(.*)<', _)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confirm if the html elements have been removed and we only have the source left\n",
    "# This is done by checking unique values in the 'source column'\n",
    "\n",
    "merged_df['source'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #8:\n",
    "\n",
    "The **`doggo`**, **`floofer`**, **`pupper`** and **`puppo`** columns on the `twitter_archive` dataset could actually be Melt into a single column and called say, 'dog_type'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some dogs that doesnt have a dog type\n",
    "\n",
    "merged_df[['doggo', 'floofer', 'pupper', 'puppo']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are dogs that have 'None' as values for all the dog types (doggo, floofer, pupper and puppo)\n",
    "# Create a function to add dogs without a dog type to a single column for identification purpose\n",
    "\n",
    "def no_dog_type(dog):\n",
    "    if dog['doggo'] == 'None' and dog['floofer'] == 'None' and dog['pupper'] == 'None' and dog['puppo'] == 'None':\n",
    "        value = 'none_dog_type'\n",
    "    else:\n",
    "        value ='None'\n",
    "    return value\n",
    "\n",
    "merged_df['unknown_dog_type'] = merged_df.apply(no_dog_type, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df[['name', 'doggo', 'floofer', 'pupper', 'puppo', 'unknown_dog_type']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the 'doggo', 'floofer', 'pupper' and 'puppo' columns are Melt into a single column, 'dog_type'\n",
    "merged_df = pd.melt(merged_df, id_vars =['tweet_id', 'timestamp', 'source', 'text', 'expanded_urls',\n",
    "       'rating_numerator', 'rating_denominator', 'name', 'jpg_url', 'img_num', 'p1', 'p1_conf', 'p1_dog',\n",
    "       'p2', 'p2_conf', 'p2_dog', 'p3', 'p3_conf', 'p3_dog', 'retweet_count',\n",
    "       'favorite_count'],\n",
    "                     value_vars = ['doggo', 'floofer', 'pupper', 'puppo', 'unknown_dog_type'],\n",
    "                     var_name = 'dog_type', \n",
    "                    value_name = 'value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all duplicated rows and drop the 'value' column\n",
    "\n",
    "merged_df = merged_df[merged_df['value']!= 'None']\n",
    "merged_df = merged_df.drop('value', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confirm if all steps applied worked\n",
    "\n",
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #9:\n",
    "\n",
    "Inconsistent case in the **`p1`**, **`p2`** and **`p3`** columns as some are lower case while some are sentence case.\n",
    "\n",
    "This is a quality issue that will be cleaned by making all the values in all three columns to be lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data to see the varying cases in the 'p1', 'p2' and 'p3' columns.\n",
    "\n",
    "merged_df[['p1', 'p2', 'p3']].iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean-up the issue by applying lowercase to all values in the 'p1', 'p2' and 'p3' columns\n",
    "\n",
    "merged_df[['p1', 'p2', 'p3']] = merged_df[['p1', 'p2', 'p3']].apply(lambda x: x.str.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Confirm this has been cleaned up\n",
    "\n",
    "merged_df[['p1', 'p2', 'p3']].iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #10:\n",
    "\n",
    "Rename columns appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data to investigate the columns and determine which ones need\n",
    "# to be renamed\n",
    "\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.rename(columns = {'timestamp':'tweet_date',\n",
    "                           'source': 'tweet_source',\n",
    "                           'text': 'tweet_text',\n",
    "                           'expanded_urls': 'tweet_url',\n",
    "                           'name': 'name_of_dog',\n",
    "                           'jpg_url': 'image_link',\n",
    "                           'breed': 'dog_breed'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the 'source', 'text', 'expanded_urls', 'name', 'jpg_url' and breed column names\n",
    "# to more descriptive names\n",
    "merged_df = merged_df.rename(columns = {'timestamp':'tweet_date',\n",
    "                           'source': 'tweet_source',\n",
    "                           'text': 'tweet_text',\n",
    "                           'expanded_urls': 'tweet_url',\n",
    "                           'name': 'name_of_dog',\n",
    "                           'jpg_url': 'image_link',\n",
    "                           'breed': 'dog_breed'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm this has been cleaned up\n",
    "\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data\n",
    "\n",
    "The `merged_df` DataFrame that has been worked on so far is stored to a CSV file named \"twitter_archive_master.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"twitter_archive_master.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing and Visualizing Data\n",
    "\n",
    "In this section, the wrangled data is analyzed and visualized by producing insights and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file containing the wrangled data\n",
    "\n",
    "df = pd.read_csv(\"twitter_archive_master.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "1. Which dogs had the highest ratings/scores\n",
    "\n",
    "2. Which dogs had the highest favourites (likes)\n",
    "\n",
    "3. Which dog_type had the highest favourites (likes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Which dogs had the highest ratings/scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('name_of_dog')['rating_numerator'].mean().sort_values(ascending=False).nlargest(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Atticus is the winner here! Atticus had the highest average score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot bar chart of the highest rated 10 dogs\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "df.groupby('name_of_dog')['rating_numerator'].mean().sort_values(ascending=False).nlargest(10).plot(kind='bar')\n",
    "plt.title(\"Top 10 Highest Rated Dogs\",fontsize=15)\n",
    "plt.ylabel(\"Dog Name\")\n",
    "plt.xlabel(\"Average Ratings\"); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Which dogs had the highest favourites (likes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby('name_of_dog')['favorite_count'].mean().sort_values(ascending=False).nlargest(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Stephan is the winner on this one, and is closely followed by Jamesy.<br><br>Now, we all like Stephan ðŸ¥°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bar chart of the highest liked 10 dogs\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "df.groupby('name_of_dog')['favorite_count'].mean().sort_values(ascending=False).nlargest(10).plot(kind='bar')\n",
    "plt.title(\"Top 10 Highest Liked Dogs\",fontsize=15)\n",
    "plt.ylabel(\"Dog Name\")\n",
    "plt.xlabel(\"Average Favourite Count\"); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3. Which dog_type had the highest favourites (likes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exclude where dog_type = 'unknown_dog_type' to make the analysis cleaner\n",
    "\n",
    "df = df[df['dog_type'] != 'unknown_dog_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.groupby('dog_type')['favorite_count'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Puppo wins the race! I'll go get a puppo...<br><br>That's a cute dogðŸ¥°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a pie chart to show how dog types faired against each other\n",
    "\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "df.groupby('dog_type')['favorite_count'].mean().sort_values(ascending=False).plot(kind='pie')\n",
    "plt.title(\"Top Liked Dog Types\",fontsize=15)\n",
    "plt.ylabel(\"Dog Type\")\n",
    "plt.xlabel(\"Average Favourite Count\");"
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "report_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
